{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2b2819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0911a37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)\n",
    "# This is a control character with no visible representation — \n",
    "# it's often used as a string terminator in C-style strings\n",
    "# or as padding/null bytes in binary formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb70318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c22b779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9556d004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62cd9192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3ad9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a03d878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294967296"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1462e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "utf32_encoded = test_string.encode(\"utf-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94f28cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf8 length: 23\n",
      "utf16 length: 28\n",
      "utf32 length: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"utf8 length: {len(utf8_encoded)}\")\n",
    "print(f\"utf16 length: {len(utf16_encoded)}\")\n",
    "print(f\"utf32 length: {len(utf32_encoded)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc03b46e",
   "metadata": {},
   "source": [
    "utf-8  = 1 byte per ASCII character\n",
    "UTF-16 = 2 bytes\n",
    "UTF-32 = 4 bytes\n",
    "UTF-8 would be more space efficient for most common words, e.g. english that are mostly ASCII.\n",
    "UTF-8 has a smaller vocabulary. 256 vs 65k vs. 4294967296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c882a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea8c4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e55e66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m decode_utf8_bytes_to_str_wrong(\u001b[33m\"\u001b[39m\u001b[33mhello! こんにちは!\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49a87e0d",
   "metadata": {},
   "source": [
    "Some Unicode characters are encoded into multiple UTF-8 byte values. The UTF-8 vocabulary is only 256, so it's not possible each of the 150k+ Unicode characters to map to one UTF-8 byte. Therefore, you cannot decode a UTF-8 byte sequence by 1 byte at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "934650f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '–' (U+2013) (4254768052.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbytes([0xC2–0xDF] [0x80–0xBF])\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '–' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "bytes([0xC2–0xDF] [0x80–0xBF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e0912c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mbytes\u001b[39m([\u001b[32m0xC0\u001b[39m, \u001b[32m0xAF\u001b[39m]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "bytes([0xC0, 0xAF]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f8d76ea",
   "metadata": {},
   "source": [
    "Why it's invalid:\n",
    "0xC0 is a 2-byte start byte.\n",
    "\n",
    "0xAF is a continuation byte.\n",
    "\n",
    "Together, they represent the code point U+002F ('/') — but this should be encoded in 1 byte (0x2F) in UTF-8.\n",
    "\n",
    "Therefore, this is an overlong encoding, which is forbidden in UTF-8.\n",
    "\n",
    "\n",
    "Pre-tokenization:\n",
    "- coarse-grain tokenization over corpus. e.g. split on white-space to get a series of words. Or, can use the following regex expression\n",
    "\n",
    "then convert each token to bytes by \"token\".encode(\"utf-8\"). and count the frequency of adjacent bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f35343",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c05adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb755ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bb7ecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(0, 4), match='some'>,\n",
       " <regex.Match object; span=(4, 9), match=' text'>,\n",
       " <regex.Match object; span=(9, 14), match=' that'>,\n",
       " <regex.Match object; span=(14, 16), match=' i'>,\n",
       " <regex.Match object; span=(16, 19), match=\"'ll\">,\n",
       " <regex.Match object; span=(19, 23), match=' pre'>,\n",
       " <regex.Match object; span=(23, 24), match='-'>,\n",
       " <regex.Match object; span=(24, 32), match='tokenize'>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(re.finditer(PAT, \"some text that i'll pre-tokenize\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b86d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokens = re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "562e76a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'some', b' text', b' that', b' i', b\"'ll\", b' pre', b'-', b'tokenize']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_token_bytes = [x.encode(\"utf-8\") for x in pre_tokens]\n",
    "pre_token_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5f7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE training example\n",
    "text = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8d445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'newest': 6, 'low': 5, 'widest': 3, 'lower': 2})\n"
     ]
    }
   ],
   "source": [
    "pre_tokens = [x.strip() for x in text.split(\" \")]\n",
    "pre_token_freqs = Counter(pre_tokens)\n",
    "print(pre_token_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3bcb641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w'): 5,\n",
       " ('l', 'o', 'w', 'e', 'r'): 2,\n",
       " ('w', 'i', 'd', 'e', 's', 't'): 3,\n",
       " ('n', 'e', 'w', 'e', 's', 't'): 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_token_byte_freqs = {tuple([c for c in key]) : count for key,count in pre_token_freqs.items() }\n",
    "pre_token_byte_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d68687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_freqs(pre_token_byte_freqs):\n",
    "    char_pair_freqs = Counter()\n",
    "    for char_sequence, freq in pre_token_byte_freqs.items():\n",
    "        for char_pair in zip(char_sequence, char_sequence[1:]):\n",
    "            char_pair_freqs[char_pair] += freq\n",
    "    return char_pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892bc450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge [0]\n",
      "pre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'e', 's', 't'): 3, ('n', 'e', 'w', 'e', 's', 't'): 6}\n",
      "\tmerge: st\n",
      "\tpre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'e', 'st'): 3, ('n', 'e', 'w', 'e', 'st'): 6}\n",
      "Merge [1]\n",
      "pre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'e', 'st'): 3, ('n', 'e', 'w', 'e', 'st'): 6}\n",
      "\tmerge: est\n",
      "\tpre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "Merge [2]\n",
      "pre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "\tmerge: ow\n",
      "\tpre_token_byte_freqs: {('l', 'ow'): 5, ('l', 'ow', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "Merge [3]\n",
      "pre_token_byte_freqs: {('l', 'ow'): 5, ('l', 'ow', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "\tmerge: low\n",
      "\tpre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "Merge [4]\n",
      "pre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "\tmerge: west\n",
      "\tpre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'west'): 6}\n",
      "Merge [5]\n",
      "pre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'west'): 6}\n",
      "\tmerge: ne\n",
      "\tpre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('ne', 'west'): 6}\n"
     ]
    }
   ],
   "source": [
    "def update_byte_freq_with_max(pre_token_byte_freqs, max_char_pair):\n",
    "    pre_token_byte_freqs_updated = {}\n",
    "    for char_sequence, freq in pre_token_byte_freqs.items():\n",
    "        char_seq_updated = []\n",
    "        i = 0\n",
    "        while i < len(char_sequence):\n",
    "            if i == len(char_sequence) - 1:\n",
    "                char_seq_updated.append(char_sequence[i])\n",
    "                break\n",
    "            char_pair = (char_sequence[i], char_sequence[i+1])        \n",
    "            if char_pair == max_char_pair:\n",
    "                char_seq_updated.append(\"\".join(max_char_pair))\n",
    "                i += 1\n",
    "            else:\n",
    "                char_seq_updated.append(char_sequence[i])\n",
    "            i += 1\n",
    "        pre_token_byte_freqs_updated[tuple(char_seq_updated)] = freq\n",
    "    return pre_token_byte_freqs_updated\n",
    "\n",
    "def merge_update(pre_token_byte_freqs):\n",
    "    char_pair_freqs = get_pair_freqs(pre_token_byte_freqs)\n",
    "    # x[1] is the freq count of the char pair.\n",
    "    # break ties by the lexicographically greater (e.g. alphabetically pair) wins.\n",
    "    max_char_pair = max(char_pair_freqs.items(), key = lambda x: (x[1], x[0]))[0]\n",
    "    pre_token_byte_freqs_updated = update_byte_freq_with_max(pre_token_byte_freqs, max_char_pair)\n",
    "    return pre_token_byte_freqs_updated, \"\".join(max_char_pair)\n",
    "\n",
    "\n",
    "merges = []\n",
    "for i in range(6):\n",
    "    print(f\"Merge [{i}]\\npre_token_byte_freqs: {pre_token_byte_freqs}\")\n",
    "    pre_token_byte_freqs, merged_chars = merge_update(pre_token_byte_freqs)\n",
    "    print(f\"\\tmerge: {merged_chars}\\n\\tpre_token_byte_freqs: {pre_token_byte_freqs}\")\n",
    "\n",
    "    merges.append(merged_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c491972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b055bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/media/bryan/ssd01/data/cs336\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca26f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fpath = os.path.join(data_dir, \"TinyStoriesV2-GPT4-valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4747847",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_dataset_fpath, 'r', encoding='utf-8') as f:\n",
    "    train_dataset_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34f6340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n<|endoftext|>\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high a'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36aa93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<|endoftext|>\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c28ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_on_special_tokens(text, special_tokens = (\"<|endoftext|>\",)):\n",
    "    delimiter = re.escape(\"|\".join(special_tokens))\n",
    "    return re.split(delimiter, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3baf9ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u\\\\ don\\'t\\\\ have\\\\ to\\\\ be\\\\ scared\\\\ of\\\\ the\\\\ loud\\\\ dog,\\\\ I\\'ll\\\\ protect\\\\ you\"\\\\.\\\\ The\\\\ mole\\\\ felt\\\\ so\\\\ safe\\\\ with\\\\ the\\\\ little\\\\ girl\\\\.\\\\ She\\\\ was\\\\ very\\\\ kind\\\\ and\\\\ the\\\\ mole\\\\ soon\\\\ came\\\\ to\\\\ trust\\\\ her\\\\.\\\\ He\\\\ leaned\\\\ against\\\\ her\\\\ and\\\\ she\\\\ kept\\\\ him\\\\ safe\\\\.\\\\ The\\\\ mole\\\\ had\\\\ found\\\\ his\\\\ best\\\\ friend\\\\.\\\\\\n<\\\\|endoftext\\\\|>\\\\\\nOnce\\\\ upon\\\\ a\\\\ time,\\\\ in\\\\ a\\\\ warm\\\\ and\\\\ sunny\\\\ place,'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.escape(train_dataset_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8047c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = split_text_on_special_tokens(train_dataset_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f189be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n',\n",
       " '\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high a']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a33c403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 84, 104, 101, 121)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(map(int, \" They\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c73d22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "84\n",
      "104\n",
      "101\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "encoded_bytes = \" They\".encode(\"utf-8\")\n",
    "for i in range(len(encoded_bytes)):\n",
    "    print(encoded_bytes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19d64e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30ba110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Th'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[84] + vocab[104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75a85b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([86]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23b9bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 124, 101, 110, 100, 111, 102, 116, 101, 120, 116, 124, 62]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, \"<|endoftext|>\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d387251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
