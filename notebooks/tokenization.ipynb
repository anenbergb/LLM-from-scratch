{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d2b2819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0911a37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)\n",
    "# This is a control character with no visible representation — \n",
    "# it's often used as a string terminator in C-style strings\n",
    "# or as padding/null bytes in binary formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb70318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c22b779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9556d004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62cd9192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3ad9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a03d878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294967296"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1462e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "utf32_encoded = test_string.encode(\"utf-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94f28cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf8 length: 23\n",
      "utf16 length: 28\n",
      "utf32 length: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"utf8 length: {len(utf8_encoded)}\")\n",
    "print(f\"utf16 length: {len(utf16_encoded)}\")\n",
    "print(f\"utf32 length: {len(utf32_encoded)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc03b46e",
   "metadata": {},
   "source": [
    "utf-8  = 1 byte per ASCII character\n",
    "UTF-16 = 2 bytes\n",
    "UTF-32 = 4 bytes\n",
    "UTF-8 would be more space efficient for most common words, e.g. english that are mostly ASCII.\n",
    "UTF-8 has a smaller vocabulary. 256 vs 65k vs. 4294967296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c882a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea8c4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e55e66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m decode_utf8_bytes_to_str_wrong(\u001b[33m\"\u001b[39m\u001b[33mhello! こんにちは!\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49a87e0d",
   "metadata": {},
   "source": [
    "Some Unicode characters are encoded into multiple UTF-8 byte values. The UTF-8 vocabulary is only 256, so it's not possible each of the 150k+ Unicode characters to map to one UTF-8 byte. Therefore, you cannot decode a UTF-8 byte sequence by 1 byte at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "934650f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '–' (U+2013) (4254768052.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbytes([0xC2–0xDF] [0x80–0xBF])\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '–' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "bytes([0xC2–0xDF] [0x80–0xBF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e0912c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mbytes\u001b[39m([\u001b[32m0xC0\u001b[39m, \u001b[32m0xAF\u001b[39m]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "bytes([0xC0, 0xAF]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f8d76ea",
   "metadata": {},
   "source": [
    "Why it's invalid:\n",
    "0xC0 is a 2-byte start byte.\n",
    "\n",
    "0xAF is a continuation byte.\n",
    "\n",
    "Together, they represent the code point U+002F ('/') — but this should be encoded in 1 byte (0x2F) in UTF-8.\n",
    "\n",
    "Therefore, this is an overlong encoding, which is forbidden in UTF-8.\n",
    "\n",
    "\n",
    "Pre-tokenization:\n",
    "- coarse-grain tokenization over corpus. e.g. split on white-space to get a series of words. Or, can use the following regex expression\n",
    "\n",
    "then convert each token to bytes by \"token\".encode(\"utf-8\"). and count the frequency of adjacent bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f35343",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c05adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb755ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bb7ecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(0, 4), match='some'>,\n",
       " <regex.Match object; span=(4, 9), match=' text'>,\n",
       " <regex.Match object; span=(9, 14), match=' that'>,\n",
       " <regex.Match object; span=(14, 16), match=' i'>,\n",
       " <regex.Match object; span=(16, 19), match=\"'ll\">,\n",
       " <regex.Match object; span=(19, 23), match=' pre'>,\n",
       " <regex.Match object; span=(23, 24), match='-'>,\n",
       " <regex.Match object; span=(24, 32), match='tokenize'>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(re.finditer(PAT, \"some text that i'll pre-tokenize\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b86d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokens = re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "562e76a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'some', b' text', b' that', b' i', b\"'ll\", b' pre', b'-', b'tokenize']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_token_bytes = [x.encode(\"utf-8\") for x in pre_tokens]\n",
    "pre_token_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5f7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE training example\n",
    "text = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8d445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'newest': 6, 'low': 5, 'widest': 3, 'lower': 2})\n"
     ]
    }
   ],
   "source": [
    "pre_tokens = [x.strip() for x in text.split(\" \")]\n",
    "pre_token_freqs = Counter(pre_tokens)\n",
    "print(pre_token_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3bcb641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w'): 5,\n",
       " ('l', 'o', 'w', 'e', 'r'): 2,\n",
       " ('w', 'i', 'd', 'e', 's', 't'): 3,\n",
       " ('n', 'e', 'w', 'e', 's', 't'): 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_token_byte_freqs = {tuple([c for c in key]) : count for key,count in pre_token_freqs.items() }\n",
    "pre_token_byte_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d68687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_freqs(pre_token_byte_freqs):\n",
    "    char_pair_freqs = Counter()\n",
    "    for char_sequence, freq in pre_token_byte_freqs.items():\n",
    "        for char_pair in zip(char_sequence, char_sequence[1:]):\n",
    "            char_pair_freqs[char_pair] += freq\n",
    "    return char_pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892bc450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge [0]\n",
      "pre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'e', 's', 't'): 3, ('n', 'e', 'w', 'e', 's', 't'): 6}\n",
      "\tmerge: st\n",
      "\tpre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'e', 'st'): 3, ('n', 'e', 'w', 'e', 'st'): 6}\n",
      "Merge [1]\n",
      "pre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'e', 'st'): 3, ('n', 'e', 'w', 'e', 'st'): 6}\n",
      "\tmerge: est\n",
      "\tpre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "Merge [2]\n",
      "pre_token_byte_freqs: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "\tmerge: ow\n",
      "\tpre_token_byte_freqs: {('l', 'ow'): 5, ('l', 'ow', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "Merge [3]\n",
      "pre_token_byte_freqs: {('l', 'ow'): 5, ('l', 'ow', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "\tmerge: low\n",
      "\tpre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "Merge [4]\n",
      "pre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'w', 'est'): 6}\n",
      "\tmerge: west\n",
      "\tpre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'west'): 6}\n",
      "Merge [5]\n",
      "pre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('n', 'e', 'west'): 6}\n",
      "\tmerge: ne\n",
      "\tpre_token_byte_freqs: {('low',): 5, ('low', 'e', 'r'): 2, ('w', 'i', 'd', 'est'): 3, ('ne', 'west'): 6}\n"
     ]
    }
   ],
   "source": [
    "def update_byte_freq_with_max(pre_token_byte_freqs, max_char_pair):\n",
    "    pre_token_byte_freqs_updated = {}\n",
    "    for char_sequence, freq in pre_token_byte_freqs.items():\n",
    "        char_seq_updated = []\n",
    "        i = 0\n",
    "        while i < len(char_sequence):\n",
    "            if i == len(char_sequence) - 1:\n",
    "                char_seq_updated.append(char_sequence[i])\n",
    "                break\n",
    "            char_pair = (char_sequence[i], char_sequence[i+1])        \n",
    "            if char_pair == max_char_pair:\n",
    "                char_seq_updated.append(\"\".join(max_char_pair))\n",
    "                i += 1\n",
    "            else:\n",
    "                char_seq_updated.append(char_sequence[i])\n",
    "            i += 1\n",
    "        pre_token_byte_freqs_updated[tuple(char_seq_updated)] = freq\n",
    "    return pre_token_byte_freqs_updated\n",
    "\n",
    "def merge_update(pre_token_byte_freqs):\n",
    "    char_pair_freqs = get_pair_freqs(pre_token_byte_freqs)\n",
    "    # x[1] is the freq count of the char pair.\n",
    "    # break ties by the lexicographically greater (e.g. alphabetically pair) wins.\n",
    "    max_char_pair = max(char_pair_freqs.items(), key = lambda x: (x[1], x[0]))[0]\n",
    "    pre_token_byte_freqs_updated = update_byte_freq_with_max(pre_token_byte_freqs, max_char_pair)\n",
    "    return pre_token_byte_freqs_updated, \"\".join(max_char_pair)\n",
    "\n",
    "\n",
    "merges = []\n",
    "for i in range(6):\n",
    "    print(f\"Merge [{i}]\\npre_token_byte_freqs: {pre_token_byte_freqs}\")\n",
    "    pre_token_byte_freqs, merged_chars = merge_update(pre_token_byte_freqs)\n",
    "    print(f\"\\tmerge: {merged_chars}\\n\\tpre_token_byte_freqs: {pre_token_byte_freqs}\")\n",
    "\n",
    "    merges.append(merged_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c491972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b055bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/media/bryan/ssd01/data/cs336\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca26f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fpath = os.path.join(data_dir, \"TinyStoriesV2-GPT4-valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4747847",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_dataset_fpath, 'r', encoding='utf-8') as f:\n",
    "    train_dataset_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34f6340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n<|endoftext|>\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high a'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36aa93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<|endoftext|>\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c28ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_on_special_tokens(text, special_tokens = (\"<|endoftext|>\",)):\n",
    "    delimiter = re.escape(\"|\".join(special_tokens))\n",
    "    return re.split(delimiter, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3baf9ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u\\\\ don\\'t\\\\ have\\\\ to\\\\ be\\\\ scared\\\\ of\\\\ the\\\\ loud\\\\ dog,\\\\ I\\'ll\\\\ protect\\\\ you\"\\\\.\\\\ The\\\\ mole\\\\ felt\\\\ so\\\\ safe\\\\ with\\\\ the\\\\ little\\\\ girl\\\\.\\\\ She\\\\ was\\\\ very\\\\ kind\\\\ and\\\\ the\\\\ mole\\\\ soon\\\\ came\\\\ to\\\\ trust\\\\ her\\\\.\\\\ He\\\\ leaned\\\\ against\\\\ her\\\\ and\\\\ she\\\\ kept\\\\ him\\\\ safe\\\\.\\\\ The\\\\ mole\\\\ had\\\\ found\\\\ his\\\\ best\\\\ friend\\\\.\\\\\\n<\\\\|endoftext\\\\|>\\\\\\nOnce\\\\ upon\\\\ a\\\\ time,\\\\ in\\\\ a\\\\ warm\\\\ and\\\\ sunny\\\\ place,'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.escape(train_dataset_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8047c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = split_text_on_special_tokens(train_dataset_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f189be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n',\n",
       " '\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high a']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a33c403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 84, 104, 101, 121)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(map(int, \" They\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c73d22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "84\n",
      "104\n",
      "101\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "encoded_bytes = \" They\".encode(\"utf-8\")\n",
    "for i in range(len(encoded_bytes)):\n",
    "    print(encoded_bytes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19d64e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30ba110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Th'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[84] + vocab[104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75a85b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([86]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23b9bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 124, 101, 110, 100, 111, 102, 116, 101, 120, 116, 124, 62]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, \"<|endoftext|>\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d387251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "083ebb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_10k_tinystories.pkl\", \"rb\") as f:\n",
    "    bpe_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8693f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab', 'merges'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bdf9cf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe_data[\"vocab\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ddbb87c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token: b' accomplishment', length: 15\n"
     ]
    }
   ],
   "source": [
    "max_token_len = 0\n",
    "max_token = None\n",
    "for token in bpe_data[\"vocab\"].values():\n",
    "    if len(token) > max_token_len:\n",
    "        max_token_len = len(token)\n",
    "        max_token = token\n",
    "\n",
    "print(f\"max_token: {max_token}, length: {max_token_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a2c9e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b' ', b't'),\n",
       " (b'h', b'e'),\n",
       " (b' ', b'a'),\n",
       " (b' ', b's'),\n",
       " (b' ', b'w'),\n",
       " (b'n', b'd'),\n",
       " (b' t', b'he'),\n",
       " (b'e', b'd'),\n",
       " (b' ', b'b'),\n",
       " (b' t', b'o')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bpe_data[\"merges\"][:10]\n",
    "\n",
    "with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "080dd3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_32k_owt_train.pkl\", \"rb\") as f:\n",
    "    bpe_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd316fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token: b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82', length: 64\n"
     ]
    }
   ],
   "source": [
    "max_token_len = 0\n",
    "max_token = None\n",
    "for token in bpe_data[\"vocab\"].values():\n",
    "    if len(token) > max_token_len:\n",
    "        max_token_len = len(token)\n",
    "        max_token = token\n",
    "\n",
    "print(f\"max_token: {max_token}, length: {max_token_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d9accdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "def pickle_as_text(pkl_file_path: str):\n",
    "    dirname = os.path.dirname(pkl_file_path)\n",
    "    basename = os.path.splitext(os.path.basename(pkl_file_path))[0]\n",
    "    print(f\"Loading {pkl_file_path}\")\n",
    "    with open(pkl_file_path, \"rb\") as f:\n",
    "        bpe_data = pickle.load(f)\n",
    "    \n",
    "    for key, data in bpe_data.items():\n",
    "        file_path = os.path.join(dirname, f\"{basename}_{key}.json\")\n",
    "        print(f\"Saving {key} to {file_path}\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b6a8065f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_10k_tinystories.pkl\n",
      "Saving vocab to /media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_10k_tinystories_vocab.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type bytes is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pickle_as_text(\u001b[33m\"\u001b[39m\u001b[33m/media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_10k_tinystories.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpickle_as_text\u001b[39m\u001b[34m(pkl_file_path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     json.dump(data, f, indent=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.12/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[32m    180\u001b[39m     fp.write(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.12/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.12/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = _default(o)\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.12/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type bytes is not JSON serializable"
     ]
    }
   ],
   "source": [
    "pickle_as_text(\"/media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_10k_tinystories.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9beab5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = bpe_data[\"vocab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4d0601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_to_token_id = {v:k for k,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03c8515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is some text', '<|endoftext|>', 'followed by ', 'SPECIAL', 'more text', '<|endoftext|>', 'end.']\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "special_tokens = [\"<|endoftext|>\", \"SPECIAL\"]\n",
    "text = \"This is some text<|endoftext|>followed by SPECIALmore text<|endoftext|>end.\"\n",
    "\n",
    "# Escape special characters in tokens and join into a regex pattern\n",
    "pattern = \"(\" + \"|\".join(map(regex.escape, special_tokens)) + \")\"\n",
    "\n",
    "# Split the text while keeping the delimiters\n",
    "chunks = regex.split(pattern, text)\n",
    "\n",
    "# Optionally remove empty strings (e.g., from split at the beginning)\n",
    "chunks = [chunk for chunk in chunks if chunk]\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "587fe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cc28be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes1, bytes2 = bpe_data[\"merges\"][-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3939431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'iskers'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "29fe68a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes2[:3] == b\"isk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8257907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bytes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a413ca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'sk'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes2[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7b7235e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[105, 115, 107, 101, 114, 115]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = list(bytes2)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5d2695a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token[0] == bytes2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "43aac421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[105, 115, 107, 101, 114, 115]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, bytes2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "56d15134",
   "metadata": {},
   "outputs": [],
   "source": [
    "token  =  b\"bytes\"\n",
    "A = list(token)\n",
    "B = list(map(int, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ea65860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[2:5] = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c1b12ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bytes = \"hello! こんにちは!\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "acdaba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e93d1f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e8342d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_char = '\\uFFFD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7bb238e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(replacement_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f0cb898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_char = chr(0xFFFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1d7d631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(replacement_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c86df2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😀'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x1F600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4bc8c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "PRETOKENIZATION_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "82fcfb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some text<|endoftext|>followed by SPECIALmore text<|endoftext|>end.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6bc7712e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " ' is',\n",
       " ' some',\n",
       " ' text',\n",
       " '<|',\n",
       " 'endoftext',\n",
       " '|>',\n",
       " 'followed',\n",
       " ' by',\n",
       " ' SPECIALmore',\n",
       " ' text',\n",
       " '<|',\n",
       " 'endoftext',\n",
       " '|>',\n",
       " 'end',\n",
       " '.']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PRETOKENIZATION_REGEX, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4d00e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes1 = b's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d8a97ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes1_list = list(bytes1)\n",
    "bytes1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7f4d3617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b's'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(bytes1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1f99362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'b', b'y', b't', b'e', b's']\n"
     ]
    }
   ],
   "source": [
    "token = b\"bytes\"\n",
    "split_bytes = [bytes([b]) for b in token]\n",
    "print(split_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4614ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "121\n",
      "116\n",
      "101\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "for b in token:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b1ac9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/bryan/src/LLM-from-scratch/tests/fixtures/tinystories_sample.txt\", \"r\") as f:\n",
    "#     for text in f:\n",
    "#         print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a0298d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'\\xFF'.decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b03171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
