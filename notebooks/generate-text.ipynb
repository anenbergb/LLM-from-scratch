{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b612fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaab1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llm.transformer import TransformerLM\n",
    "from llm.tokenization import Tokenizer\n",
    "from llm.serialization import load_checkpoint\n",
    "from llm.generation import generateLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e651ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_pkl = \"/media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_10k_tinystories.pkl\"\n",
    "# tokenized_dataset_pkl = \"/media/bryan/ssd01/expr/llm_from_scratch/tokenization/bpe_32k_owt_train.pkl\"\n",
    "eos_token = \"<|endoftext|>\"\n",
    "tokenizer = Tokenizer.from_pickle(tokenized_dataset_pkl, special_tokens=[eos_token])\n",
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6490f5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pth = \"/media/bryan/ssd01/expr/llm_from_scratch/tune-batch-size/lr1e-3-bs256-iters50k/checkpoint_best.pt\"\n",
    "model = TransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=256,\n",
    "    num_layers=4,\n",
    "    num_heads=16,\n",
    "    d_model=512,\n",
    "    d_ff=1344,\n",
    "    rope_theta=10000,\n",
    ")\n",
    "\n",
    "# model_pth = \"/media/bryan/ssd01/expr/llm_from_scratch/owl-model-size/medium-500k-lr5e-4/checkpoint_best.pt\"\n",
    "# model = TransformerLM(\n",
    "#     vocab_size=vocab_size,\n",
    "#     context_length=256,\n",
    "#     num_layers=24,\n",
    "#     num_heads=16,\n",
    "#     d_model=1024,\n",
    "#     d_ff=4096,\n",
    "#     rope_theta=10000,\n",
    "# )\n",
    "checkpoint = torch.load(model_pth)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99eee149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (token_embeddings): Embedding(vocab_size=10000, d=512)\n",
       "  (RoPE): RotaryPositionalEmbedding(context_length=256, dim/2=16)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (attn): CausalMHSARoPE(\n",
       "        (qkv_proj): Linear(d_out=1536, d_in=512)\n",
       "        (output_proj): Linear(d_out=512, d_in=512)\n",
       "        (RoPE): RotaryPositionalEmbedding(context_length=256, dim/2=16)\n",
       "      )\n",
       "      (ffn): SwiGLU(\n",
       "        (w1): Linear(d_out=1344, d_in=512)\n",
       "        (w2): Linear(d_out=512, d_in=1344)\n",
       "        (w3): Linear(d_out=1344, d_in=512)\n",
       "      )\n",
       "      (ln1): RMSNorm(hidden_size=512, eps=1e-05)\n",
       "      (ln2): RMSNorm(hidden_size=512, eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNorm(hidden_size=512, eps=1e-05)\n",
       "  (lm_head): Linear(d_out=10000, d_in=512)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval();\n",
    "model.to(\"cuda\");\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5a55704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "Once upon a time, there was a pretty girl named Lily. She loved to eat\n",
      "GENERATED:\n",
      "Once upon a time, there was a pretty girl named Lily. She loved to eat pizza while she played games and shared her toys with her friends. One day, she decided to make a special menu for her friends.\n",
      "Lily called her friends Tom and Lily and said, \"Come see the menu! We have pizza!\" Tom and Lily came and saw the menu. They all wanted to eat cheese for their lunch. Lily's mom gave them some pizza, and they all sat down to eat.\n",
      "After they were done eating, Lily said, \"I will record the yummy food on my plate. I am sure my friends will love it.\" She held her plate and started to record. They all smiled and said, \"Thank you for the yummy food!\" Lily was happy to share her yummy food with her friends.\n",
      "<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPTS = [\n",
    "#     \"Once upon a time there was a little boy named Ben. Ben loved to\",\n",
    "   \"Once upon a time, there was a pretty girl named Lily. She loved to eat\",\n",
    "#     \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. One of the key algorithms in supervised learning is\"\n",
    "]\n",
    "for prompt in PROMPTS:\n",
    "    generated_text = generateLLM(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=1,\n",
    "        top_k=25,\n",
    "        top_p=0,\n",
    "        eos_token=eos_token,\n",
    "        seed=11,\n",
    "    )\n",
    "    print_text = f\"PROMPT:\\n{prompt}\\nGENERATED:\\n{prompt}{generated_text}\\n\"\n",
    "    print(print_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
