{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f5042b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b22f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from einops import einsum, rearrange\n",
    "import numpy as np\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from llm.transformer import TransformerLM\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from fvcore.nn import parameter_count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef7e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "# context_length = 16384\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "d_ff = 6400\n",
    "model = TransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    d_model=d_model,\n",
    "    d_ff = d_ff,\n",
    "    rope_theta = 1000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae65a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (token_embeddings): Embedding(vocab_size=50257, d=1600)\n",
       "  (RoPE): RotaryPositionalEmbedding(context_length=1024, dim/2=32)\n",
       "  (layers): ModuleList(\n",
       "    (0-47): 48 x TransformerBlock(\n",
       "      (attn): CausalMHSARoPE(\n",
       "        (qkv_proj): Linear(d_out=4800, d_in=1600)\n",
       "        (output_proj): Linear(d_out=1600, d_in=1600)\n",
       "        (RoPE): RotaryPositionalEmbedding(context_length=1024, dim/2=32)\n",
       "      )\n",
       "      (ffn): SwiGLU(\n",
       "        (w1): Linear(d_out=6400, d_in=1600)\n",
       "        (w2): Linear(d_out=1600, d_in=6400)\n",
       "        (w3): Linear(d_out=6400, d_in=1600)\n",
       "      )\n",
       "      (ln1): RMSNorm(hidden_size=1600, eps=1e-05)\n",
       "      (ln2): RMSNorm(hidden_size=1600, eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNorm(hidden_size=1600, eps=1e-05)\n",
       "  (lm_head): Linear(d_out=50257, d_in=1600)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be343067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127057600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427815ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters_in_millions(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {total_params / 1e6:.2f}M\")\n",
    "def parameter_memory_in_megabytes(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_bytes = total_params * 4  # float32 = 4 bytes\n",
    "    total_megabytes = total_bytes / (1024 ** 2)  # bytes to MB\n",
    "    print(f\"Estimated memory (float32): {total_megabytes:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee02b32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2127.06M\n",
      "Estimated memory (float32): 8114.08 MB\n"
     ]
    }
   ],
   "source": [
    "count_parameters_in_millions(model)\n",
    "parameter_memory_in_megabytes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9831898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::pow encountered 97 time(s)\n",
      "Unsupported operator aten::mean encountered 97 time(s)\n",
      "Unsupported operator aten::add encountered 289 time(s)\n",
      "Unsupported operator aten::sqrt encountered 97 time(s)\n",
      "Unsupported operator aten::div encountered 145 time(s)\n",
      "Unsupported operator aten::mul encountered 625 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 192 time(s)\n",
      "Unsupported operator aten::neg encountered 96 time(s)\n",
      "Unsupported operator aten::reshape_as encountered 96 time(s)\n",
      "Unsupported operator aten::tril encountered 48 time(s)\n",
      "Unsupported operator aten::div_ encountered 48 time(s)\n",
      "Unsupported operator aten::sub encountered 48 time(s)\n",
      "Unsupported operator aten::exp encountered 48 time(s)\n",
      "Unsupported operator aten::sum encountered 48 time(s)\n",
      "Unsupported operator aten::mul_ encountered 48 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 48 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 2256.58 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    in_indices = torch.zeros(1, context_length, dtype=torch.int64)\n",
    "    flops = FlopCountAnalysis(model, in_indices)\n",
    "    print(f\"FLOPs: {flops.total()/1e9:.2f} GFLOPs\")  # in billions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3898fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count table:\n",
      "| name                      | #elements or shape   |\n",
      "|:--------------------------|:---------------------|\n",
      "| model                     | 2.1G                 |\n",
      "|  token_embeddings         |  80.4M               |\n",
      "|   token_embeddings.weight |   (50257, 1600)      |\n",
      "|  layers                   |  2.0G                |\n",
      "|   layers.0                |   41.0M              |\n",
      "|   layers.1                |   41.0M              |\n",
      "|   layers.2                |   41.0M              |\n",
      "|   layers.3                |   41.0M              |\n",
      "|   layers.4                |   41.0M              |\n",
      "|   layers.5                |   41.0M              |\n",
      "|   layers.6                |   41.0M              |\n",
      "|   layers.7                |   41.0M              |\n",
      "|   layers.8                |   41.0M              |\n",
      "|   layers.9                |   41.0M              |\n",
      "|   layers.10               |   41.0M              |\n",
      "|   layers.11               |   41.0M              |\n",
      "|   layers.12               |   41.0M              |\n",
      "|   layers.13               |   41.0M              |\n",
      "|   layers.14               |   41.0M              |\n",
      "|   layers.15               |   41.0M              |\n",
      "|   layers.16               |   41.0M              |\n",
      "|   layers.17               |   41.0M              |\n",
      "|   layers.18               |   41.0M              |\n",
      "|   layers.19               |   41.0M              |\n",
      "|   layers.20               |   41.0M              |\n",
      "|   layers.21               |   41.0M              |\n",
      "|   layers.22               |   41.0M              |\n",
      "|   layers.23               |   41.0M              |\n",
      "|   layers.24               |   41.0M              |\n",
      "|   layers.25               |   41.0M              |\n",
      "|   layers.26               |   41.0M              |\n",
      "|   layers.27               |   41.0M              |\n",
      "|   layers.28               |   41.0M              |\n",
      "|   layers.29               |   41.0M              |\n",
      "|   layers.30               |   41.0M              |\n",
      "|   layers.31               |   41.0M              |\n",
      "|   layers.32               |   41.0M              |\n",
      "|   layers.33               |   41.0M              |\n",
      "|   layers.34               |   41.0M              |\n",
      "|   layers.35               |   41.0M              |\n",
      "|   layers.36               |   41.0M              |\n",
      "|   layers.37               |   41.0M              |\n",
      "|   layers.38               |   41.0M              |\n",
      "|   layers.39               |   41.0M              |\n",
      "|   layers.40               |   41.0M              |\n",
      "|   layers.41               |   41.0M              |\n",
      "|   layers.42               |   41.0M              |\n",
      "|   layers.43               |   41.0M              |\n",
      "|   layers.44               |   41.0M              |\n",
      "|   layers.45               |   41.0M              |\n",
      "|   layers.46               |   41.0M              |\n",
      "|   layers.47               |   41.0M              |\n",
      "|  ln_final                 |  1.6K                |\n",
      "|   ln_final.weight         |   (1600,)            |\n",
      "|  lm_head                  |  80.4M               |\n",
      "|   lm_head.weight          |   (50257, 1600)      |\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter count table:\")\n",
    "print(parameter_count_table(model, max_depth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a895c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 2256.58 GFLOPs\n",
      "layers: 2174.23 GFLOPs\n",
      "layers.0: 45.30 GFLOPs\n",
      "layers.0.attn: 13.84 GFLOPs\n",
      "layers.0.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.0.attn.output_proj: 2.62 GFLOPs\n",
      "layers.0.ffn: 31.45 GFLOPs\n",
      "layers.0.ffn.w1: 10.48 GFLOPs\n",
      "layers.0.ffn.w2: 10.48 GFLOPs\n",
      "layers.0.ffn.w3: 10.48 GFLOPs\n",
      "layers.1: 45.30 GFLOPs\n",
      "layers.1.attn: 13.84 GFLOPs\n",
      "layers.1.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.1.attn.output_proj: 2.62 GFLOPs\n",
      "layers.1.ffn: 31.45 GFLOPs\n",
      "layers.1.ffn.w1: 10.48 GFLOPs\n",
      "layers.1.ffn.w2: 10.48 GFLOPs\n",
      "layers.1.ffn.w3: 10.48 GFLOPs\n",
      "layers.2: 45.30 GFLOPs\n",
      "layers.2.attn: 13.84 GFLOPs\n",
      "layers.2.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.2.attn.output_proj: 2.62 GFLOPs\n",
      "layers.2.ffn: 31.45 GFLOPs\n",
      "layers.2.ffn.w1: 10.48 GFLOPs\n",
      "layers.2.ffn.w2: 10.48 GFLOPs\n",
      "layers.2.ffn.w3: 10.48 GFLOPs\n",
      "layers.3: 45.30 GFLOPs\n",
      "layers.3.attn: 13.84 GFLOPs\n",
      "layers.3.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.3.attn.output_proj: 2.62 GFLOPs\n",
      "layers.3.ffn: 31.45 GFLOPs\n",
      "layers.3.ffn.w1: 10.48 GFLOPs\n",
      "layers.3.ffn.w2: 10.48 GFLOPs\n",
      "layers.3.ffn.w3: 10.48 GFLOPs\n",
      "layers.4: 45.30 GFLOPs\n",
      "layers.4.attn: 13.84 GFLOPs\n",
      "layers.4.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.4.attn.output_proj: 2.62 GFLOPs\n",
      "layers.4.ffn: 31.45 GFLOPs\n",
      "layers.4.ffn.w1: 10.48 GFLOPs\n",
      "layers.4.ffn.w2: 10.48 GFLOPs\n",
      "layers.4.ffn.w3: 10.48 GFLOPs\n",
      "layers.5: 45.30 GFLOPs\n",
      "layers.5.attn: 13.84 GFLOPs\n",
      "layers.5.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.5.attn.output_proj: 2.62 GFLOPs\n",
      "layers.5.ffn: 31.45 GFLOPs\n",
      "layers.5.ffn.w1: 10.48 GFLOPs\n",
      "layers.5.ffn.w2: 10.48 GFLOPs\n",
      "layers.5.ffn.w3: 10.48 GFLOPs\n",
      "layers.6: 45.30 GFLOPs\n",
      "layers.6.attn: 13.84 GFLOPs\n",
      "layers.6.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.6.attn.output_proj: 2.62 GFLOPs\n",
      "layers.6.ffn: 31.45 GFLOPs\n",
      "layers.6.ffn.w1: 10.48 GFLOPs\n",
      "layers.6.ffn.w2: 10.48 GFLOPs\n",
      "layers.6.ffn.w3: 10.48 GFLOPs\n",
      "layers.7: 45.30 GFLOPs\n",
      "layers.7.attn: 13.84 GFLOPs\n",
      "layers.7.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.7.attn.output_proj: 2.62 GFLOPs\n",
      "layers.7.ffn: 31.45 GFLOPs\n",
      "layers.7.ffn.w1: 10.48 GFLOPs\n",
      "layers.7.ffn.w2: 10.48 GFLOPs\n",
      "layers.7.ffn.w3: 10.48 GFLOPs\n",
      "layers.8: 45.30 GFLOPs\n",
      "layers.8.attn: 13.84 GFLOPs\n",
      "layers.8.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.8.attn.output_proj: 2.62 GFLOPs\n",
      "layers.8.ffn: 31.45 GFLOPs\n",
      "layers.8.ffn.w1: 10.48 GFLOPs\n",
      "layers.8.ffn.w2: 10.48 GFLOPs\n",
      "layers.8.ffn.w3: 10.48 GFLOPs\n",
      "layers.9: 45.30 GFLOPs\n",
      "layers.9.attn: 13.84 GFLOPs\n",
      "layers.9.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.9.attn.output_proj: 2.62 GFLOPs\n",
      "layers.9.ffn: 31.45 GFLOPs\n",
      "layers.9.ffn.w1: 10.48 GFLOPs\n",
      "layers.9.ffn.w2: 10.48 GFLOPs\n",
      "layers.9.ffn.w3: 10.48 GFLOPs\n",
      "layers.10: 45.30 GFLOPs\n",
      "layers.10.attn: 13.84 GFLOPs\n",
      "layers.10.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.10.attn.output_proj: 2.62 GFLOPs\n",
      "layers.10.ffn: 31.45 GFLOPs\n",
      "layers.10.ffn.w1: 10.48 GFLOPs\n",
      "layers.10.ffn.w2: 10.48 GFLOPs\n",
      "layers.10.ffn.w3: 10.48 GFLOPs\n",
      "layers.11: 45.30 GFLOPs\n",
      "layers.11.attn: 13.84 GFLOPs\n",
      "layers.11.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.11.attn.output_proj: 2.62 GFLOPs\n",
      "layers.11.ffn: 31.45 GFLOPs\n",
      "layers.11.ffn.w1: 10.48 GFLOPs\n",
      "layers.11.ffn.w2: 10.48 GFLOPs\n",
      "layers.11.ffn.w3: 10.48 GFLOPs\n",
      "layers.12: 45.30 GFLOPs\n",
      "layers.12.attn: 13.84 GFLOPs\n",
      "layers.12.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.12.attn.output_proj: 2.62 GFLOPs\n",
      "layers.12.ffn: 31.45 GFLOPs\n",
      "layers.12.ffn.w1: 10.48 GFLOPs\n",
      "layers.12.ffn.w2: 10.48 GFLOPs\n",
      "layers.12.ffn.w3: 10.48 GFLOPs\n",
      "layers.13: 45.30 GFLOPs\n",
      "layers.13.attn: 13.84 GFLOPs\n",
      "layers.13.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.13.attn.output_proj: 2.62 GFLOPs\n",
      "layers.13.ffn: 31.45 GFLOPs\n",
      "layers.13.ffn.w1: 10.48 GFLOPs\n",
      "layers.13.ffn.w2: 10.48 GFLOPs\n",
      "layers.13.ffn.w3: 10.48 GFLOPs\n",
      "layers.14: 45.30 GFLOPs\n",
      "layers.14.attn: 13.84 GFLOPs\n",
      "layers.14.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.14.attn.output_proj: 2.62 GFLOPs\n",
      "layers.14.ffn: 31.45 GFLOPs\n",
      "layers.14.ffn.w1: 10.48 GFLOPs\n",
      "layers.14.ffn.w2: 10.48 GFLOPs\n",
      "layers.14.ffn.w3: 10.48 GFLOPs\n",
      "layers.15: 45.30 GFLOPs\n",
      "layers.15.attn: 13.84 GFLOPs\n",
      "layers.15.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.15.attn.output_proj: 2.62 GFLOPs\n",
      "layers.15.ffn: 31.45 GFLOPs\n",
      "layers.15.ffn.w1: 10.48 GFLOPs\n",
      "layers.15.ffn.w2: 10.48 GFLOPs\n",
      "layers.15.ffn.w3: 10.48 GFLOPs\n",
      "layers.16: 45.30 GFLOPs\n",
      "layers.16.attn: 13.84 GFLOPs\n",
      "layers.16.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.16.attn.output_proj: 2.62 GFLOPs\n",
      "layers.16.ffn: 31.45 GFLOPs\n",
      "layers.16.ffn.w1: 10.48 GFLOPs\n",
      "layers.16.ffn.w2: 10.48 GFLOPs\n",
      "layers.16.ffn.w3: 10.48 GFLOPs\n",
      "layers.17: 45.30 GFLOPs\n",
      "layers.17.attn: 13.84 GFLOPs\n",
      "layers.17.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.17.attn.output_proj: 2.62 GFLOPs\n",
      "layers.17.ffn: 31.45 GFLOPs\n",
      "layers.17.ffn.w1: 10.48 GFLOPs\n",
      "layers.17.ffn.w2: 10.48 GFLOPs\n",
      "layers.17.ffn.w3: 10.48 GFLOPs\n",
      "layers.18: 45.30 GFLOPs\n",
      "layers.18.attn: 13.84 GFLOPs\n",
      "layers.18.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.18.attn.output_proj: 2.62 GFLOPs\n",
      "layers.18.ffn: 31.45 GFLOPs\n",
      "layers.18.ffn.w1: 10.48 GFLOPs\n",
      "layers.18.ffn.w2: 10.48 GFLOPs\n",
      "layers.18.ffn.w3: 10.48 GFLOPs\n",
      "layers.19: 45.30 GFLOPs\n",
      "layers.19.attn: 13.84 GFLOPs\n",
      "layers.19.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.19.attn.output_proj: 2.62 GFLOPs\n",
      "layers.19.ffn: 31.45 GFLOPs\n",
      "layers.19.ffn.w1: 10.48 GFLOPs\n",
      "layers.19.ffn.w2: 10.48 GFLOPs\n",
      "layers.19.ffn.w3: 10.48 GFLOPs\n",
      "layers.20: 45.30 GFLOPs\n",
      "layers.20.attn: 13.84 GFLOPs\n",
      "layers.20.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.20.attn.output_proj: 2.62 GFLOPs\n",
      "layers.20.ffn: 31.45 GFLOPs\n",
      "layers.20.ffn.w1: 10.48 GFLOPs\n",
      "layers.20.ffn.w2: 10.48 GFLOPs\n",
      "layers.20.ffn.w3: 10.48 GFLOPs\n",
      "layers.21: 45.30 GFLOPs\n",
      "layers.21.attn: 13.84 GFLOPs\n",
      "layers.21.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.21.attn.output_proj: 2.62 GFLOPs\n",
      "layers.21.ffn: 31.45 GFLOPs\n",
      "layers.21.ffn.w1: 10.48 GFLOPs\n",
      "layers.21.ffn.w2: 10.48 GFLOPs\n",
      "layers.21.ffn.w3: 10.48 GFLOPs\n",
      "layers.22: 45.30 GFLOPs\n",
      "layers.22.attn: 13.84 GFLOPs\n",
      "layers.22.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.22.attn.output_proj: 2.62 GFLOPs\n",
      "layers.22.ffn: 31.45 GFLOPs\n",
      "layers.22.ffn.w1: 10.48 GFLOPs\n",
      "layers.22.ffn.w2: 10.48 GFLOPs\n",
      "layers.22.ffn.w3: 10.48 GFLOPs\n",
      "layers.23: 45.30 GFLOPs\n",
      "layers.23.attn: 13.84 GFLOPs\n",
      "layers.23.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.23.attn.output_proj: 2.62 GFLOPs\n",
      "layers.23.ffn: 31.45 GFLOPs\n",
      "layers.23.ffn.w1: 10.48 GFLOPs\n",
      "layers.23.ffn.w2: 10.48 GFLOPs\n",
      "layers.23.ffn.w3: 10.48 GFLOPs\n",
      "layers.24: 45.30 GFLOPs\n",
      "layers.24.attn: 13.84 GFLOPs\n",
      "layers.24.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.24.attn.output_proj: 2.62 GFLOPs\n",
      "layers.24.ffn: 31.45 GFLOPs\n",
      "layers.24.ffn.w1: 10.48 GFLOPs\n",
      "layers.24.ffn.w2: 10.48 GFLOPs\n",
      "layers.24.ffn.w3: 10.48 GFLOPs\n",
      "layers.25: 45.30 GFLOPs\n",
      "layers.25.attn: 13.84 GFLOPs\n",
      "layers.25.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.25.attn.output_proj: 2.62 GFLOPs\n",
      "layers.25.ffn: 31.45 GFLOPs\n",
      "layers.25.ffn.w1: 10.48 GFLOPs\n",
      "layers.25.ffn.w2: 10.48 GFLOPs\n",
      "layers.25.ffn.w3: 10.48 GFLOPs\n",
      "layers.26: 45.30 GFLOPs\n",
      "layers.26.attn: 13.84 GFLOPs\n",
      "layers.26.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.26.attn.output_proj: 2.62 GFLOPs\n",
      "layers.26.ffn: 31.45 GFLOPs\n",
      "layers.26.ffn.w1: 10.48 GFLOPs\n",
      "layers.26.ffn.w2: 10.48 GFLOPs\n",
      "layers.26.ffn.w3: 10.48 GFLOPs\n",
      "layers.27: 45.30 GFLOPs\n",
      "layers.27.attn: 13.84 GFLOPs\n",
      "layers.27.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.27.attn.output_proj: 2.62 GFLOPs\n",
      "layers.27.ffn: 31.45 GFLOPs\n",
      "layers.27.ffn.w1: 10.48 GFLOPs\n",
      "layers.27.ffn.w2: 10.48 GFLOPs\n",
      "layers.27.ffn.w3: 10.48 GFLOPs\n",
      "layers.28: 45.30 GFLOPs\n",
      "layers.28.attn: 13.84 GFLOPs\n",
      "layers.28.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.28.attn.output_proj: 2.62 GFLOPs\n",
      "layers.28.ffn: 31.45 GFLOPs\n",
      "layers.28.ffn.w1: 10.48 GFLOPs\n",
      "layers.28.ffn.w2: 10.48 GFLOPs\n",
      "layers.28.ffn.w3: 10.48 GFLOPs\n",
      "layers.29: 45.30 GFLOPs\n",
      "layers.29.attn: 13.84 GFLOPs\n",
      "layers.29.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.29.attn.output_proj: 2.62 GFLOPs\n",
      "layers.29.ffn: 31.45 GFLOPs\n",
      "layers.29.ffn.w1: 10.48 GFLOPs\n",
      "layers.29.ffn.w2: 10.48 GFLOPs\n",
      "layers.29.ffn.w3: 10.48 GFLOPs\n",
      "layers.30: 45.30 GFLOPs\n",
      "layers.30.attn: 13.84 GFLOPs\n",
      "layers.30.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.30.attn.output_proj: 2.62 GFLOPs\n",
      "layers.30.ffn: 31.45 GFLOPs\n",
      "layers.30.ffn.w1: 10.48 GFLOPs\n",
      "layers.30.ffn.w2: 10.48 GFLOPs\n",
      "layers.30.ffn.w3: 10.48 GFLOPs\n",
      "layers.31: 45.30 GFLOPs\n",
      "layers.31.attn: 13.84 GFLOPs\n",
      "layers.31.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.31.attn.output_proj: 2.62 GFLOPs\n",
      "layers.31.ffn: 31.45 GFLOPs\n",
      "layers.31.ffn.w1: 10.48 GFLOPs\n",
      "layers.31.ffn.w2: 10.48 GFLOPs\n",
      "layers.31.ffn.w3: 10.48 GFLOPs\n",
      "layers.32: 45.30 GFLOPs\n",
      "layers.32.attn: 13.84 GFLOPs\n",
      "layers.32.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.32.attn.output_proj: 2.62 GFLOPs\n",
      "layers.32.ffn: 31.45 GFLOPs\n",
      "layers.32.ffn.w1: 10.48 GFLOPs\n",
      "layers.32.ffn.w2: 10.48 GFLOPs\n",
      "layers.32.ffn.w3: 10.48 GFLOPs\n",
      "layers.33: 45.30 GFLOPs\n",
      "layers.33.attn: 13.84 GFLOPs\n",
      "layers.33.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.33.attn.output_proj: 2.62 GFLOPs\n",
      "layers.33.ffn: 31.45 GFLOPs\n",
      "layers.33.ffn.w1: 10.48 GFLOPs\n",
      "layers.33.ffn.w2: 10.48 GFLOPs\n",
      "layers.33.ffn.w3: 10.48 GFLOPs\n",
      "layers.34: 45.30 GFLOPs\n",
      "layers.34.attn: 13.84 GFLOPs\n",
      "layers.34.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.34.attn.output_proj: 2.62 GFLOPs\n",
      "layers.34.ffn: 31.45 GFLOPs\n",
      "layers.34.ffn.w1: 10.48 GFLOPs\n",
      "layers.34.ffn.w2: 10.48 GFLOPs\n",
      "layers.34.ffn.w3: 10.48 GFLOPs\n",
      "layers.35: 45.30 GFLOPs\n",
      "layers.35.attn: 13.84 GFLOPs\n",
      "layers.35.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.35.attn.output_proj: 2.62 GFLOPs\n",
      "layers.35.ffn: 31.45 GFLOPs\n",
      "layers.35.ffn.w1: 10.48 GFLOPs\n",
      "layers.35.ffn.w2: 10.48 GFLOPs\n",
      "layers.35.ffn.w3: 10.48 GFLOPs\n",
      "layers.36: 45.30 GFLOPs\n",
      "layers.36.attn: 13.84 GFLOPs\n",
      "layers.36.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.36.attn.output_proj: 2.62 GFLOPs\n",
      "layers.36.ffn: 31.45 GFLOPs\n",
      "layers.36.ffn.w1: 10.48 GFLOPs\n",
      "layers.36.ffn.w2: 10.48 GFLOPs\n",
      "layers.36.ffn.w3: 10.48 GFLOPs\n",
      "layers.37: 45.30 GFLOPs\n",
      "layers.37.attn: 13.84 GFLOPs\n",
      "layers.37.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.37.attn.output_proj: 2.62 GFLOPs\n",
      "layers.37.ffn: 31.45 GFLOPs\n",
      "layers.37.ffn.w1: 10.48 GFLOPs\n",
      "layers.37.ffn.w2: 10.48 GFLOPs\n",
      "layers.37.ffn.w3: 10.48 GFLOPs\n",
      "layers.38: 45.30 GFLOPs\n",
      "layers.38.attn: 13.84 GFLOPs\n",
      "layers.38.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.38.attn.output_proj: 2.62 GFLOPs\n",
      "layers.38.ffn: 31.45 GFLOPs\n",
      "layers.38.ffn.w1: 10.48 GFLOPs\n",
      "layers.38.ffn.w2: 10.48 GFLOPs\n",
      "layers.38.ffn.w3: 10.48 GFLOPs\n",
      "layers.39: 45.30 GFLOPs\n",
      "layers.39.attn: 13.84 GFLOPs\n",
      "layers.39.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.39.attn.output_proj: 2.62 GFLOPs\n",
      "layers.39.ffn: 31.45 GFLOPs\n",
      "layers.39.ffn.w1: 10.48 GFLOPs\n",
      "layers.39.ffn.w2: 10.48 GFLOPs\n",
      "layers.39.ffn.w3: 10.48 GFLOPs\n",
      "layers.40: 45.30 GFLOPs\n",
      "layers.40.attn: 13.84 GFLOPs\n",
      "layers.40.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.40.attn.output_proj: 2.62 GFLOPs\n",
      "layers.40.ffn: 31.45 GFLOPs\n",
      "layers.40.ffn.w1: 10.48 GFLOPs\n",
      "layers.40.ffn.w2: 10.48 GFLOPs\n",
      "layers.40.ffn.w3: 10.48 GFLOPs\n",
      "layers.41: 45.30 GFLOPs\n",
      "layers.41.attn: 13.84 GFLOPs\n",
      "layers.41.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.41.attn.output_proj: 2.62 GFLOPs\n",
      "layers.41.ffn: 31.45 GFLOPs\n",
      "layers.41.ffn.w1: 10.48 GFLOPs\n",
      "layers.41.ffn.w2: 10.48 GFLOPs\n",
      "layers.41.ffn.w3: 10.48 GFLOPs\n",
      "layers.42: 45.30 GFLOPs\n",
      "layers.42.attn: 13.84 GFLOPs\n",
      "layers.42.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.42.attn.output_proj: 2.62 GFLOPs\n",
      "layers.42.ffn: 31.45 GFLOPs\n",
      "layers.42.ffn.w1: 10.48 GFLOPs\n",
      "layers.42.ffn.w2: 10.48 GFLOPs\n",
      "layers.42.ffn.w3: 10.48 GFLOPs\n",
      "layers.43: 45.30 GFLOPs\n",
      "layers.43.attn: 13.84 GFLOPs\n",
      "layers.43.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.43.attn.output_proj: 2.62 GFLOPs\n",
      "layers.43.ffn: 31.45 GFLOPs\n",
      "layers.43.ffn.w1: 10.48 GFLOPs\n",
      "layers.43.ffn.w2: 10.48 GFLOPs\n",
      "layers.43.ffn.w3: 10.48 GFLOPs\n",
      "layers.44: 45.30 GFLOPs\n",
      "layers.44.attn: 13.84 GFLOPs\n",
      "layers.44.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.44.attn.output_proj: 2.62 GFLOPs\n",
      "layers.44.ffn: 31.45 GFLOPs\n",
      "layers.44.ffn.w1: 10.48 GFLOPs\n",
      "layers.44.ffn.w2: 10.48 GFLOPs\n",
      "layers.44.ffn.w3: 10.48 GFLOPs\n",
      "layers.45: 45.30 GFLOPs\n",
      "layers.45.attn: 13.84 GFLOPs\n",
      "layers.45.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.45.attn.output_proj: 2.62 GFLOPs\n",
      "layers.45.ffn: 31.45 GFLOPs\n",
      "layers.45.ffn.w1: 10.48 GFLOPs\n",
      "layers.45.ffn.w2: 10.48 GFLOPs\n",
      "layers.45.ffn.w3: 10.48 GFLOPs\n",
      "layers.46: 45.30 GFLOPs\n",
      "layers.46.attn: 13.84 GFLOPs\n",
      "layers.46.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.46.attn.output_proj: 2.62 GFLOPs\n",
      "layers.46.ffn: 31.45 GFLOPs\n",
      "layers.46.ffn.w1: 10.48 GFLOPs\n",
      "layers.46.ffn.w2: 10.48 GFLOPs\n",
      "layers.46.ffn.w3: 10.48 GFLOPs\n",
      "layers.47: 45.30 GFLOPs\n",
      "layers.47.attn: 13.84 GFLOPs\n",
      "layers.47.attn.qkv_proj: 7.87 GFLOPs\n",
      "layers.47.attn.output_proj: 2.62 GFLOPs\n",
      "layers.47.ffn: 31.45 GFLOPs\n",
      "layers.47.ffn.w1: 10.48 GFLOPs\n",
      "layers.47.ffn.w2: 10.48 GFLOPs\n",
      "layers.47.ffn.w3: 10.48 GFLOPs\n",
      "lm_head: 82.35 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "for name, num_flops in flops.by_module().items():\n",
    "    gflops = f\"{num_flops/1e9:.2f} GFLOPs\"\n",
    "    if name == \"\":\n",
    "        name = \"TOTAL\"\n",
    "    if num_flops > 0:\n",
    "        print(f\"{name}: {gflops}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6e3c9fc",
   "metadata": {},
   "source": [
    "Each transformer layer requires 45.3 GFLOPs\n",
    "- within a given layer, the FFN requires the majority (31.45 GFLOPs), evenly broken down into 3x 10.48 GFLOPs per Linear layer\n",
    "\n",
    "The final Linear layer (LM_head) makes up a larger fraction of the overall FLOPs for the small models\n",
    "- 22.61% for GPT-2 small\n",
    "- 10.20% for GPT-2 medium\n",
    "- 5.83% for GPT-large\n",
    "- 3.79% for GPT-2 XL\n",
    "\n",
    "If the context length is increased to 16,384 the total FLOPs for the GPT-2 XL model increases to 74759.66 GFLOPs\n",
    "The final linear layer (LM_head) makes up 1.76% of total FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd187663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head FLOPs ratio: 3.79%\n"
     ]
    }
   ],
   "source": [
    "print(f\"lm_head FLOPs ratio: {100 * 82.35 / 2174.23:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24333df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Q@K^T 3.36 GFLOPs\n",
      "Attention * V^T 3.36 GFLOPs\n",
      "Scaling by 1/sqrt(d_k): 0.03 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# (1,25,1024,64) @ (1,25,1024,64)\n",
    "# FLOPs = 2 * 1024 * 1024 * 64 for the (1024,64) @ (1024,64) multiplication\n",
    "# FLOPs across batch = 1*25*FLOPs from above\n",
    "flops_attn_QV =  num_heads * (2 * context_length * context_length * (d_model / num_heads))\n",
    "print(f\"Attention Q@K^T {flops_attn_QV/1e9:.2f} GFLOPs\")\n",
    "# (1,25,1024,1024) * (1,25,1024,64)^T = (1,25,1024,64)\n",
    "flops_attn_out = num_heads * (2 * context_length * context_length* (d_model / num_heads))\n",
    "print(f\"Attention * V^T {flops_attn_out/1e9:.2f} GFLOPs\")\n",
    "\n",
    "sqrt_flops = num_heads * (context_length**2)\n",
    "print(f\"Scaling by 1/sqrt(d_k): {sqrt_flops/1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c65c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = TransformerLM(\n",
    "    vocab_size=50257,\n",
    "    context_length=1024,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    d_model=768,\n",
    "    d_ff = 768*4,\n",
    "    rope_theta = 1000.0\n",
    ")\n",
    "model_med = TransformerLM(\n",
    "    vocab_size=50257,\n",
    "    context_length=1024,\n",
    "    num_layers=24,\n",
    "    num_heads=16,\n",
    "    d_model=1024,\n",
    "    d_ff = 1024*4,\n",
    "    rope_theta = 1000.0\n",
    ")\n",
    "model_large = TransformerLM(\n",
    "    vocab_size=50257,\n",
    "    context_length=1024,\n",
    "    num_layers=36,\n",
    "    num_heads=20,\n",
    "    d_model=1280,\n",
    "    d_ff = 1280*4,\n",
    "    rope_theta = 1000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085e7b07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::pow encountered 25 time(s)\n",
      "Unsupported operator aten::mean encountered 25 time(s)\n",
      "Unsupported operator aten::add encountered 73 time(s)\n",
      "Unsupported operator aten::sqrt encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 37 time(s)\n",
      "Unsupported operator aten::mul encountered 157 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 48 time(s)\n",
      "Unsupported operator aten::neg encountered 24 time(s)\n",
      "Unsupported operator aten::reshape_as encountered 24 time(s)\n",
      "Unsupported operator aten::tril encountered 12 time(s)\n",
      "Unsupported operator aten::div_ encountered 12 time(s)\n",
      "Unsupported operator aten::sub encountered 12 time(s)\n",
      "Unsupported operator aten::exp encountered 12 time(s)\n",
      "Unsupported operator aten::sum encountered 12 time(s)\n",
      "Unsupported operator aten::mul_ encountered 12 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 12 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 174.82 GFLOPs\n",
      "lm_head FLOPs ratio: 22.61%\n",
      "TOTAL: 174.82 GFLOPs\n",
      "layers: 135.30 GFLOPs\n",
      "layers.0: 11.28 GFLOPs\n",
      "layers.0.attn: 4.03 GFLOPs\n",
      "layers.0.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.0.attn.output_proj: 0.60 GFLOPs\n",
      "layers.0.ffn: 7.25 GFLOPs\n",
      "layers.0.ffn.w1: 2.42 GFLOPs\n",
      "layers.0.ffn.w2: 2.42 GFLOPs\n",
      "layers.0.ffn.w3: 2.42 GFLOPs\n",
      "layers.1: 11.28 GFLOPs\n",
      "layers.1.attn: 4.03 GFLOPs\n",
      "layers.1.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.1.attn.output_proj: 0.60 GFLOPs\n",
      "layers.1.ffn: 7.25 GFLOPs\n",
      "layers.1.ffn.w1: 2.42 GFLOPs\n",
      "layers.1.ffn.w2: 2.42 GFLOPs\n",
      "layers.1.ffn.w3: 2.42 GFLOPs\n",
      "layers.2: 11.28 GFLOPs\n",
      "layers.2.attn: 4.03 GFLOPs\n",
      "layers.2.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.2.attn.output_proj: 0.60 GFLOPs\n",
      "layers.2.ffn: 7.25 GFLOPs\n",
      "layers.2.ffn.w1: 2.42 GFLOPs\n",
      "layers.2.ffn.w2: 2.42 GFLOPs\n",
      "layers.2.ffn.w3: 2.42 GFLOPs\n",
      "layers.3: 11.28 GFLOPs\n",
      "layers.3.attn: 4.03 GFLOPs\n",
      "layers.3.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.3.attn.output_proj: 0.60 GFLOPs\n",
      "layers.3.ffn: 7.25 GFLOPs\n",
      "layers.3.ffn.w1: 2.42 GFLOPs\n",
      "layers.3.ffn.w2: 2.42 GFLOPs\n",
      "layers.3.ffn.w3: 2.42 GFLOPs\n",
      "layers.4: 11.28 GFLOPs\n",
      "layers.4.attn: 4.03 GFLOPs\n",
      "layers.4.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.4.attn.output_proj: 0.60 GFLOPs\n",
      "layers.4.ffn: 7.25 GFLOPs\n",
      "layers.4.ffn.w1: 2.42 GFLOPs\n",
      "layers.4.ffn.w2: 2.42 GFLOPs\n",
      "layers.4.ffn.w3: 2.42 GFLOPs\n",
      "layers.5: 11.28 GFLOPs\n",
      "layers.5.attn: 4.03 GFLOPs\n",
      "layers.5.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.5.attn.output_proj: 0.60 GFLOPs\n",
      "layers.5.ffn: 7.25 GFLOPs\n",
      "layers.5.ffn.w1: 2.42 GFLOPs\n",
      "layers.5.ffn.w2: 2.42 GFLOPs\n",
      "layers.5.ffn.w3: 2.42 GFLOPs\n",
      "layers.6: 11.28 GFLOPs\n",
      "layers.6.attn: 4.03 GFLOPs\n",
      "layers.6.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.6.attn.output_proj: 0.60 GFLOPs\n",
      "layers.6.ffn: 7.25 GFLOPs\n",
      "layers.6.ffn.w1: 2.42 GFLOPs\n",
      "layers.6.ffn.w2: 2.42 GFLOPs\n",
      "layers.6.ffn.w3: 2.42 GFLOPs\n",
      "layers.7: 11.28 GFLOPs\n",
      "layers.7.attn: 4.03 GFLOPs\n",
      "layers.7.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.7.attn.output_proj: 0.60 GFLOPs\n",
      "layers.7.ffn: 7.25 GFLOPs\n",
      "layers.7.ffn.w1: 2.42 GFLOPs\n",
      "layers.7.ffn.w2: 2.42 GFLOPs\n",
      "layers.7.ffn.w3: 2.42 GFLOPs\n",
      "layers.8: 11.28 GFLOPs\n",
      "layers.8.attn: 4.03 GFLOPs\n",
      "layers.8.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.8.attn.output_proj: 0.60 GFLOPs\n",
      "layers.8.ffn: 7.25 GFLOPs\n",
      "layers.8.ffn.w1: 2.42 GFLOPs\n",
      "layers.8.ffn.w2: 2.42 GFLOPs\n",
      "layers.8.ffn.w3: 2.42 GFLOPs\n",
      "layers.9: 11.28 GFLOPs\n",
      "layers.9.attn: 4.03 GFLOPs\n",
      "layers.9.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.9.attn.output_proj: 0.60 GFLOPs\n",
      "layers.9.ffn: 7.25 GFLOPs\n",
      "layers.9.ffn.w1: 2.42 GFLOPs\n",
      "layers.9.ffn.w2: 2.42 GFLOPs\n",
      "layers.9.ffn.w3: 2.42 GFLOPs\n",
      "layers.10: 11.28 GFLOPs\n",
      "layers.10.attn: 4.03 GFLOPs\n",
      "layers.10.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.10.attn.output_proj: 0.60 GFLOPs\n",
      "layers.10.ffn: 7.25 GFLOPs\n",
      "layers.10.ffn.w1: 2.42 GFLOPs\n",
      "layers.10.ffn.w2: 2.42 GFLOPs\n",
      "layers.10.ffn.w3: 2.42 GFLOPs\n",
      "layers.11: 11.28 GFLOPs\n",
      "layers.11.attn: 4.03 GFLOPs\n",
      "layers.11.attn.qkv_proj: 1.81 GFLOPs\n",
      "layers.11.attn.output_proj: 0.60 GFLOPs\n",
      "layers.11.ffn: 7.25 GFLOPs\n",
      "layers.11.ffn.w1: 2.42 GFLOPs\n",
      "layers.11.ffn.w2: 2.42 GFLOPs\n",
      "layers.11.ffn.w3: 2.42 GFLOPs\n",
      "lm_head: 39.52 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::pow encountered 49 time(s)\n",
      "Unsupported operator aten::mean encountered 49 time(s)\n",
      "Unsupported operator aten::add encountered 145 time(s)\n",
      "Unsupported operator aten::sqrt encountered 49 time(s)\n",
      "Unsupported operator aten::div encountered 73 time(s)\n",
      "Unsupported operator aten::mul encountered 313 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 96 time(s)\n",
      "Unsupported operator aten::neg encountered 48 time(s)\n",
      "Unsupported operator aten::reshape_as encountered 48 time(s)\n",
      "Unsupported operator aten::tril encountered 24 time(s)\n",
      "Unsupported operator aten::div_ encountered 24 time(s)\n",
      "Unsupported operator aten::sub encountered 24 time(s)\n",
      "Unsupported operator aten::exp encountered 24 time(s)\n",
      "Unsupported operator aten::sum encountered 24 time(s)\n",
      "Unsupported operator aten::mul_ encountered 24 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 24 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 516.54 GFLOPs\n",
      "lm_head FLOPs ratio: 10.20%\n",
      "TOTAL: 516.54 GFLOPs\n",
      "layers: 463.84 GFLOPs\n",
      "layers.0: 19.33 GFLOPs\n",
      "layers.0.attn: 6.44 GFLOPs\n",
      "layers.0.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.0.attn.output_proj: 1.07 GFLOPs\n",
      "layers.0.ffn: 12.88 GFLOPs\n",
      "layers.0.ffn.w1: 4.29 GFLOPs\n",
      "layers.0.ffn.w2: 4.29 GFLOPs\n",
      "layers.0.ffn.w3: 4.29 GFLOPs\n",
      "layers.1: 19.33 GFLOPs\n",
      "layers.1.attn: 6.44 GFLOPs\n",
      "layers.1.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.1.attn.output_proj: 1.07 GFLOPs\n",
      "layers.1.ffn: 12.88 GFLOPs\n",
      "layers.1.ffn.w1: 4.29 GFLOPs\n",
      "layers.1.ffn.w2: 4.29 GFLOPs\n",
      "layers.1.ffn.w3: 4.29 GFLOPs\n",
      "layers.2: 19.33 GFLOPs\n",
      "layers.2.attn: 6.44 GFLOPs\n",
      "layers.2.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.2.attn.output_proj: 1.07 GFLOPs\n",
      "layers.2.ffn: 12.88 GFLOPs\n",
      "layers.2.ffn.w1: 4.29 GFLOPs\n",
      "layers.2.ffn.w2: 4.29 GFLOPs\n",
      "layers.2.ffn.w3: 4.29 GFLOPs\n",
      "layers.3: 19.33 GFLOPs\n",
      "layers.3.attn: 6.44 GFLOPs\n",
      "layers.3.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.3.attn.output_proj: 1.07 GFLOPs\n",
      "layers.3.ffn: 12.88 GFLOPs\n",
      "layers.3.ffn.w1: 4.29 GFLOPs\n",
      "layers.3.ffn.w2: 4.29 GFLOPs\n",
      "layers.3.ffn.w3: 4.29 GFLOPs\n",
      "layers.4: 19.33 GFLOPs\n",
      "layers.4.attn: 6.44 GFLOPs\n",
      "layers.4.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.4.attn.output_proj: 1.07 GFLOPs\n",
      "layers.4.ffn: 12.88 GFLOPs\n",
      "layers.4.ffn.w1: 4.29 GFLOPs\n",
      "layers.4.ffn.w2: 4.29 GFLOPs\n",
      "layers.4.ffn.w3: 4.29 GFLOPs\n",
      "layers.5: 19.33 GFLOPs\n",
      "layers.5.attn: 6.44 GFLOPs\n",
      "layers.5.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.5.attn.output_proj: 1.07 GFLOPs\n",
      "layers.5.ffn: 12.88 GFLOPs\n",
      "layers.5.ffn.w1: 4.29 GFLOPs\n",
      "layers.5.ffn.w2: 4.29 GFLOPs\n",
      "layers.5.ffn.w3: 4.29 GFLOPs\n",
      "layers.6: 19.33 GFLOPs\n",
      "layers.6.attn: 6.44 GFLOPs\n",
      "layers.6.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.6.attn.output_proj: 1.07 GFLOPs\n",
      "layers.6.ffn: 12.88 GFLOPs\n",
      "layers.6.ffn.w1: 4.29 GFLOPs\n",
      "layers.6.ffn.w2: 4.29 GFLOPs\n",
      "layers.6.ffn.w3: 4.29 GFLOPs\n",
      "layers.7: 19.33 GFLOPs\n",
      "layers.7.attn: 6.44 GFLOPs\n",
      "layers.7.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.7.attn.output_proj: 1.07 GFLOPs\n",
      "layers.7.ffn: 12.88 GFLOPs\n",
      "layers.7.ffn.w1: 4.29 GFLOPs\n",
      "layers.7.ffn.w2: 4.29 GFLOPs\n",
      "layers.7.ffn.w3: 4.29 GFLOPs\n",
      "layers.8: 19.33 GFLOPs\n",
      "layers.8.attn: 6.44 GFLOPs\n",
      "layers.8.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.8.attn.output_proj: 1.07 GFLOPs\n",
      "layers.8.ffn: 12.88 GFLOPs\n",
      "layers.8.ffn.w1: 4.29 GFLOPs\n",
      "layers.8.ffn.w2: 4.29 GFLOPs\n",
      "layers.8.ffn.w3: 4.29 GFLOPs\n",
      "layers.9: 19.33 GFLOPs\n",
      "layers.9.attn: 6.44 GFLOPs\n",
      "layers.9.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.9.attn.output_proj: 1.07 GFLOPs\n",
      "layers.9.ffn: 12.88 GFLOPs\n",
      "layers.9.ffn.w1: 4.29 GFLOPs\n",
      "layers.9.ffn.w2: 4.29 GFLOPs\n",
      "layers.9.ffn.w3: 4.29 GFLOPs\n",
      "layers.10: 19.33 GFLOPs\n",
      "layers.10.attn: 6.44 GFLOPs\n",
      "layers.10.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.10.attn.output_proj: 1.07 GFLOPs\n",
      "layers.10.ffn: 12.88 GFLOPs\n",
      "layers.10.ffn.w1: 4.29 GFLOPs\n",
      "layers.10.ffn.w2: 4.29 GFLOPs\n",
      "layers.10.ffn.w3: 4.29 GFLOPs\n",
      "layers.11: 19.33 GFLOPs\n",
      "layers.11.attn: 6.44 GFLOPs\n",
      "layers.11.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.11.attn.output_proj: 1.07 GFLOPs\n",
      "layers.11.ffn: 12.88 GFLOPs\n",
      "layers.11.ffn.w1: 4.29 GFLOPs\n",
      "layers.11.ffn.w2: 4.29 GFLOPs\n",
      "layers.11.ffn.w3: 4.29 GFLOPs\n",
      "layers.12: 19.33 GFLOPs\n",
      "layers.12.attn: 6.44 GFLOPs\n",
      "layers.12.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.12.attn.output_proj: 1.07 GFLOPs\n",
      "layers.12.ffn: 12.88 GFLOPs\n",
      "layers.12.ffn.w1: 4.29 GFLOPs\n",
      "layers.12.ffn.w2: 4.29 GFLOPs\n",
      "layers.12.ffn.w3: 4.29 GFLOPs\n",
      "layers.13: 19.33 GFLOPs\n",
      "layers.13.attn: 6.44 GFLOPs\n",
      "layers.13.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.13.attn.output_proj: 1.07 GFLOPs\n",
      "layers.13.ffn: 12.88 GFLOPs\n",
      "layers.13.ffn.w1: 4.29 GFLOPs\n",
      "layers.13.ffn.w2: 4.29 GFLOPs\n",
      "layers.13.ffn.w3: 4.29 GFLOPs\n",
      "layers.14: 19.33 GFLOPs\n",
      "layers.14.attn: 6.44 GFLOPs\n",
      "layers.14.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.14.attn.output_proj: 1.07 GFLOPs\n",
      "layers.14.ffn: 12.88 GFLOPs\n",
      "layers.14.ffn.w1: 4.29 GFLOPs\n",
      "layers.14.ffn.w2: 4.29 GFLOPs\n",
      "layers.14.ffn.w3: 4.29 GFLOPs\n",
      "layers.15: 19.33 GFLOPs\n",
      "layers.15.attn: 6.44 GFLOPs\n",
      "layers.15.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.15.attn.output_proj: 1.07 GFLOPs\n",
      "layers.15.ffn: 12.88 GFLOPs\n",
      "layers.15.ffn.w1: 4.29 GFLOPs\n",
      "layers.15.ffn.w2: 4.29 GFLOPs\n",
      "layers.15.ffn.w3: 4.29 GFLOPs\n",
      "layers.16: 19.33 GFLOPs\n",
      "layers.16.attn: 6.44 GFLOPs\n",
      "layers.16.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.16.attn.output_proj: 1.07 GFLOPs\n",
      "layers.16.ffn: 12.88 GFLOPs\n",
      "layers.16.ffn.w1: 4.29 GFLOPs\n",
      "layers.16.ffn.w2: 4.29 GFLOPs\n",
      "layers.16.ffn.w3: 4.29 GFLOPs\n",
      "layers.17: 19.33 GFLOPs\n",
      "layers.17.attn: 6.44 GFLOPs\n",
      "layers.17.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.17.attn.output_proj: 1.07 GFLOPs\n",
      "layers.17.ffn: 12.88 GFLOPs\n",
      "layers.17.ffn.w1: 4.29 GFLOPs\n",
      "layers.17.ffn.w2: 4.29 GFLOPs\n",
      "layers.17.ffn.w3: 4.29 GFLOPs\n",
      "layers.18: 19.33 GFLOPs\n",
      "layers.18.attn: 6.44 GFLOPs\n",
      "layers.18.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.18.attn.output_proj: 1.07 GFLOPs\n",
      "layers.18.ffn: 12.88 GFLOPs\n",
      "layers.18.ffn.w1: 4.29 GFLOPs\n",
      "layers.18.ffn.w2: 4.29 GFLOPs\n",
      "layers.18.ffn.w3: 4.29 GFLOPs\n",
      "layers.19: 19.33 GFLOPs\n",
      "layers.19.attn: 6.44 GFLOPs\n",
      "layers.19.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.19.attn.output_proj: 1.07 GFLOPs\n",
      "layers.19.ffn: 12.88 GFLOPs\n",
      "layers.19.ffn.w1: 4.29 GFLOPs\n",
      "layers.19.ffn.w2: 4.29 GFLOPs\n",
      "layers.19.ffn.w3: 4.29 GFLOPs\n",
      "layers.20: 19.33 GFLOPs\n",
      "layers.20.attn: 6.44 GFLOPs\n",
      "layers.20.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.20.attn.output_proj: 1.07 GFLOPs\n",
      "layers.20.ffn: 12.88 GFLOPs\n",
      "layers.20.ffn.w1: 4.29 GFLOPs\n",
      "layers.20.ffn.w2: 4.29 GFLOPs\n",
      "layers.20.ffn.w3: 4.29 GFLOPs\n",
      "layers.21: 19.33 GFLOPs\n",
      "layers.21.attn: 6.44 GFLOPs\n",
      "layers.21.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.21.attn.output_proj: 1.07 GFLOPs\n",
      "layers.21.ffn: 12.88 GFLOPs\n",
      "layers.21.ffn.w1: 4.29 GFLOPs\n",
      "layers.21.ffn.w2: 4.29 GFLOPs\n",
      "layers.21.ffn.w3: 4.29 GFLOPs\n",
      "layers.22: 19.33 GFLOPs\n",
      "layers.22.attn: 6.44 GFLOPs\n",
      "layers.22.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.22.attn.output_proj: 1.07 GFLOPs\n",
      "layers.22.ffn: 12.88 GFLOPs\n",
      "layers.22.ffn.w1: 4.29 GFLOPs\n",
      "layers.22.ffn.w2: 4.29 GFLOPs\n",
      "layers.22.ffn.w3: 4.29 GFLOPs\n",
      "layers.23: 19.33 GFLOPs\n",
      "layers.23.attn: 6.44 GFLOPs\n",
      "layers.23.attn.qkv_proj: 3.22 GFLOPs\n",
      "layers.23.attn.output_proj: 1.07 GFLOPs\n",
      "layers.23.ffn: 12.88 GFLOPs\n",
      "layers.23.ffn.w1: 4.29 GFLOPs\n",
      "layers.23.ffn.w2: 4.29 GFLOPs\n",
      "layers.23.ffn.w3: 4.29 GFLOPs\n",
      "lm_head: 52.70 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::pow encountered 73 time(s)\n",
      "Unsupported operator aten::mean encountered 73 time(s)\n",
      "Unsupported operator aten::add encountered 217 time(s)\n",
      "Unsupported operator aten::sqrt encountered 73 time(s)\n",
      "Unsupported operator aten::div encountered 109 time(s)\n",
      "Unsupported operator aten::mul encountered 469 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 144 time(s)\n",
      "Unsupported operator aten::neg encountered 72 time(s)\n",
      "Unsupported operator aten::reshape_as encountered 72 time(s)\n",
      "Unsupported operator aten::tril encountered 36 time(s)\n",
      "Unsupported operator aten::div_ encountered 36 time(s)\n",
      "Unsupported operator aten::sub encountered 36 time(s)\n",
      "Unsupported operator aten::exp encountered 36 time(s)\n",
      "Unsupported operator aten::sum encountered 36 time(s)\n",
      "Unsupported operator aten::mul_ encountered 36 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 36 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 1128.80 GFLOPs\n",
      "lm_head FLOPs ratio: 5.83%\n",
      "TOTAL: 1128.80 GFLOPs\n",
      "layers: 1062.95 GFLOPs\n",
      "layers.0: 29.53 GFLOPs\n",
      "layers.0.attn: 9.40 GFLOPs\n",
      "layers.0.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.0.attn.output_proj: 1.68 GFLOPs\n",
      "layers.0.ffn: 20.13 GFLOPs\n",
      "layers.0.ffn.w1: 6.71 GFLOPs\n",
      "layers.0.ffn.w2: 6.71 GFLOPs\n",
      "layers.0.ffn.w3: 6.71 GFLOPs\n",
      "layers.1: 29.53 GFLOPs\n",
      "layers.1.attn: 9.40 GFLOPs\n",
      "layers.1.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.1.attn.output_proj: 1.68 GFLOPs\n",
      "layers.1.ffn: 20.13 GFLOPs\n",
      "layers.1.ffn.w1: 6.71 GFLOPs\n",
      "layers.1.ffn.w2: 6.71 GFLOPs\n",
      "layers.1.ffn.w3: 6.71 GFLOPs\n",
      "layers.2: 29.53 GFLOPs\n",
      "layers.2.attn: 9.40 GFLOPs\n",
      "layers.2.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.2.attn.output_proj: 1.68 GFLOPs\n",
      "layers.2.ffn: 20.13 GFLOPs\n",
      "layers.2.ffn.w1: 6.71 GFLOPs\n",
      "layers.2.ffn.w2: 6.71 GFLOPs\n",
      "layers.2.ffn.w3: 6.71 GFLOPs\n",
      "layers.3: 29.53 GFLOPs\n",
      "layers.3.attn: 9.40 GFLOPs\n",
      "layers.3.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.3.attn.output_proj: 1.68 GFLOPs\n",
      "layers.3.ffn: 20.13 GFLOPs\n",
      "layers.3.ffn.w1: 6.71 GFLOPs\n",
      "layers.3.ffn.w2: 6.71 GFLOPs\n",
      "layers.3.ffn.w3: 6.71 GFLOPs\n",
      "layers.4: 29.53 GFLOPs\n",
      "layers.4.attn: 9.40 GFLOPs\n",
      "layers.4.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.4.attn.output_proj: 1.68 GFLOPs\n",
      "layers.4.ffn: 20.13 GFLOPs\n",
      "layers.4.ffn.w1: 6.71 GFLOPs\n",
      "layers.4.ffn.w2: 6.71 GFLOPs\n",
      "layers.4.ffn.w3: 6.71 GFLOPs\n",
      "layers.5: 29.53 GFLOPs\n",
      "layers.5.attn: 9.40 GFLOPs\n",
      "layers.5.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.5.attn.output_proj: 1.68 GFLOPs\n",
      "layers.5.ffn: 20.13 GFLOPs\n",
      "layers.5.ffn.w1: 6.71 GFLOPs\n",
      "layers.5.ffn.w2: 6.71 GFLOPs\n",
      "layers.5.ffn.w3: 6.71 GFLOPs\n",
      "layers.6: 29.53 GFLOPs\n",
      "layers.6.attn: 9.40 GFLOPs\n",
      "layers.6.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.6.attn.output_proj: 1.68 GFLOPs\n",
      "layers.6.ffn: 20.13 GFLOPs\n",
      "layers.6.ffn.w1: 6.71 GFLOPs\n",
      "layers.6.ffn.w2: 6.71 GFLOPs\n",
      "layers.6.ffn.w3: 6.71 GFLOPs\n",
      "layers.7: 29.53 GFLOPs\n",
      "layers.7.attn: 9.40 GFLOPs\n",
      "layers.7.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.7.attn.output_proj: 1.68 GFLOPs\n",
      "layers.7.ffn: 20.13 GFLOPs\n",
      "layers.7.ffn.w1: 6.71 GFLOPs\n",
      "layers.7.ffn.w2: 6.71 GFLOPs\n",
      "layers.7.ffn.w3: 6.71 GFLOPs\n",
      "layers.8: 29.53 GFLOPs\n",
      "layers.8.attn: 9.40 GFLOPs\n",
      "layers.8.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.8.attn.output_proj: 1.68 GFLOPs\n",
      "layers.8.ffn: 20.13 GFLOPs\n",
      "layers.8.ffn.w1: 6.71 GFLOPs\n",
      "layers.8.ffn.w2: 6.71 GFLOPs\n",
      "layers.8.ffn.w3: 6.71 GFLOPs\n",
      "layers.9: 29.53 GFLOPs\n",
      "layers.9.attn: 9.40 GFLOPs\n",
      "layers.9.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.9.attn.output_proj: 1.68 GFLOPs\n",
      "layers.9.ffn: 20.13 GFLOPs\n",
      "layers.9.ffn.w1: 6.71 GFLOPs\n",
      "layers.9.ffn.w2: 6.71 GFLOPs\n",
      "layers.9.ffn.w3: 6.71 GFLOPs\n",
      "layers.10: 29.53 GFLOPs\n",
      "layers.10.attn: 9.40 GFLOPs\n",
      "layers.10.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.10.attn.output_proj: 1.68 GFLOPs\n",
      "layers.10.ffn: 20.13 GFLOPs\n",
      "layers.10.ffn.w1: 6.71 GFLOPs\n",
      "layers.10.ffn.w2: 6.71 GFLOPs\n",
      "layers.10.ffn.w3: 6.71 GFLOPs\n",
      "layers.11: 29.53 GFLOPs\n",
      "layers.11.attn: 9.40 GFLOPs\n",
      "layers.11.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.11.attn.output_proj: 1.68 GFLOPs\n",
      "layers.11.ffn: 20.13 GFLOPs\n",
      "layers.11.ffn.w1: 6.71 GFLOPs\n",
      "layers.11.ffn.w2: 6.71 GFLOPs\n",
      "layers.11.ffn.w3: 6.71 GFLOPs\n",
      "layers.12: 29.53 GFLOPs\n",
      "layers.12.attn: 9.40 GFLOPs\n",
      "layers.12.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.12.attn.output_proj: 1.68 GFLOPs\n",
      "layers.12.ffn: 20.13 GFLOPs\n",
      "layers.12.ffn.w1: 6.71 GFLOPs\n",
      "layers.12.ffn.w2: 6.71 GFLOPs\n",
      "layers.12.ffn.w3: 6.71 GFLOPs\n",
      "layers.13: 29.53 GFLOPs\n",
      "layers.13.attn: 9.40 GFLOPs\n",
      "layers.13.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.13.attn.output_proj: 1.68 GFLOPs\n",
      "layers.13.ffn: 20.13 GFLOPs\n",
      "layers.13.ffn.w1: 6.71 GFLOPs\n",
      "layers.13.ffn.w2: 6.71 GFLOPs\n",
      "layers.13.ffn.w3: 6.71 GFLOPs\n",
      "layers.14: 29.53 GFLOPs\n",
      "layers.14.attn: 9.40 GFLOPs\n",
      "layers.14.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.14.attn.output_proj: 1.68 GFLOPs\n",
      "layers.14.ffn: 20.13 GFLOPs\n",
      "layers.14.ffn.w1: 6.71 GFLOPs\n",
      "layers.14.ffn.w2: 6.71 GFLOPs\n",
      "layers.14.ffn.w3: 6.71 GFLOPs\n",
      "layers.15: 29.53 GFLOPs\n",
      "layers.15.attn: 9.40 GFLOPs\n",
      "layers.15.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.15.attn.output_proj: 1.68 GFLOPs\n",
      "layers.15.ffn: 20.13 GFLOPs\n",
      "layers.15.ffn.w1: 6.71 GFLOPs\n",
      "layers.15.ffn.w2: 6.71 GFLOPs\n",
      "layers.15.ffn.w3: 6.71 GFLOPs\n",
      "layers.16: 29.53 GFLOPs\n",
      "layers.16.attn: 9.40 GFLOPs\n",
      "layers.16.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.16.attn.output_proj: 1.68 GFLOPs\n",
      "layers.16.ffn: 20.13 GFLOPs\n",
      "layers.16.ffn.w1: 6.71 GFLOPs\n",
      "layers.16.ffn.w2: 6.71 GFLOPs\n",
      "layers.16.ffn.w3: 6.71 GFLOPs\n",
      "layers.17: 29.53 GFLOPs\n",
      "layers.17.attn: 9.40 GFLOPs\n",
      "layers.17.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.17.attn.output_proj: 1.68 GFLOPs\n",
      "layers.17.ffn: 20.13 GFLOPs\n",
      "layers.17.ffn.w1: 6.71 GFLOPs\n",
      "layers.17.ffn.w2: 6.71 GFLOPs\n",
      "layers.17.ffn.w3: 6.71 GFLOPs\n",
      "layers.18: 29.53 GFLOPs\n",
      "layers.18.attn: 9.40 GFLOPs\n",
      "layers.18.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.18.attn.output_proj: 1.68 GFLOPs\n",
      "layers.18.ffn: 20.13 GFLOPs\n",
      "layers.18.ffn.w1: 6.71 GFLOPs\n",
      "layers.18.ffn.w2: 6.71 GFLOPs\n",
      "layers.18.ffn.w3: 6.71 GFLOPs\n",
      "layers.19: 29.53 GFLOPs\n",
      "layers.19.attn: 9.40 GFLOPs\n",
      "layers.19.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.19.attn.output_proj: 1.68 GFLOPs\n",
      "layers.19.ffn: 20.13 GFLOPs\n",
      "layers.19.ffn.w1: 6.71 GFLOPs\n",
      "layers.19.ffn.w2: 6.71 GFLOPs\n",
      "layers.19.ffn.w3: 6.71 GFLOPs\n",
      "layers.20: 29.53 GFLOPs\n",
      "layers.20.attn: 9.40 GFLOPs\n",
      "layers.20.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.20.attn.output_proj: 1.68 GFLOPs\n",
      "layers.20.ffn: 20.13 GFLOPs\n",
      "layers.20.ffn.w1: 6.71 GFLOPs\n",
      "layers.20.ffn.w2: 6.71 GFLOPs\n",
      "layers.20.ffn.w3: 6.71 GFLOPs\n",
      "layers.21: 29.53 GFLOPs\n",
      "layers.21.attn: 9.40 GFLOPs\n",
      "layers.21.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.21.attn.output_proj: 1.68 GFLOPs\n",
      "layers.21.ffn: 20.13 GFLOPs\n",
      "layers.21.ffn.w1: 6.71 GFLOPs\n",
      "layers.21.ffn.w2: 6.71 GFLOPs\n",
      "layers.21.ffn.w3: 6.71 GFLOPs\n",
      "layers.22: 29.53 GFLOPs\n",
      "layers.22.attn: 9.40 GFLOPs\n",
      "layers.22.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.22.attn.output_proj: 1.68 GFLOPs\n",
      "layers.22.ffn: 20.13 GFLOPs\n",
      "layers.22.ffn.w1: 6.71 GFLOPs\n",
      "layers.22.ffn.w2: 6.71 GFLOPs\n",
      "layers.22.ffn.w3: 6.71 GFLOPs\n",
      "layers.23: 29.53 GFLOPs\n",
      "layers.23.attn: 9.40 GFLOPs\n",
      "layers.23.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.23.attn.output_proj: 1.68 GFLOPs\n",
      "layers.23.ffn: 20.13 GFLOPs\n",
      "layers.23.ffn.w1: 6.71 GFLOPs\n",
      "layers.23.ffn.w2: 6.71 GFLOPs\n",
      "layers.23.ffn.w3: 6.71 GFLOPs\n",
      "layers.24: 29.53 GFLOPs\n",
      "layers.24.attn: 9.40 GFLOPs\n",
      "layers.24.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.24.attn.output_proj: 1.68 GFLOPs\n",
      "layers.24.ffn: 20.13 GFLOPs\n",
      "layers.24.ffn.w1: 6.71 GFLOPs\n",
      "layers.24.ffn.w2: 6.71 GFLOPs\n",
      "layers.24.ffn.w3: 6.71 GFLOPs\n",
      "layers.25: 29.53 GFLOPs\n",
      "layers.25.attn: 9.40 GFLOPs\n",
      "layers.25.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.25.attn.output_proj: 1.68 GFLOPs\n",
      "layers.25.ffn: 20.13 GFLOPs\n",
      "layers.25.ffn.w1: 6.71 GFLOPs\n",
      "layers.25.ffn.w2: 6.71 GFLOPs\n",
      "layers.25.ffn.w3: 6.71 GFLOPs\n",
      "layers.26: 29.53 GFLOPs\n",
      "layers.26.attn: 9.40 GFLOPs\n",
      "layers.26.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.26.attn.output_proj: 1.68 GFLOPs\n",
      "layers.26.ffn: 20.13 GFLOPs\n",
      "layers.26.ffn.w1: 6.71 GFLOPs\n",
      "layers.26.ffn.w2: 6.71 GFLOPs\n",
      "layers.26.ffn.w3: 6.71 GFLOPs\n",
      "layers.27: 29.53 GFLOPs\n",
      "layers.27.attn: 9.40 GFLOPs\n",
      "layers.27.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.27.attn.output_proj: 1.68 GFLOPs\n",
      "layers.27.ffn: 20.13 GFLOPs\n",
      "layers.27.ffn.w1: 6.71 GFLOPs\n",
      "layers.27.ffn.w2: 6.71 GFLOPs\n",
      "layers.27.ffn.w3: 6.71 GFLOPs\n",
      "layers.28: 29.53 GFLOPs\n",
      "layers.28.attn: 9.40 GFLOPs\n",
      "layers.28.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.28.attn.output_proj: 1.68 GFLOPs\n",
      "layers.28.ffn: 20.13 GFLOPs\n",
      "layers.28.ffn.w1: 6.71 GFLOPs\n",
      "layers.28.ffn.w2: 6.71 GFLOPs\n",
      "layers.28.ffn.w3: 6.71 GFLOPs\n",
      "layers.29: 29.53 GFLOPs\n",
      "layers.29.attn: 9.40 GFLOPs\n",
      "layers.29.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.29.attn.output_proj: 1.68 GFLOPs\n",
      "layers.29.ffn: 20.13 GFLOPs\n",
      "layers.29.ffn.w1: 6.71 GFLOPs\n",
      "layers.29.ffn.w2: 6.71 GFLOPs\n",
      "layers.29.ffn.w3: 6.71 GFLOPs\n",
      "layers.30: 29.53 GFLOPs\n",
      "layers.30.attn: 9.40 GFLOPs\n",
      "layers.30.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.30.attn.output_proj: 1.68 GFLOPs\n",
      "layers.30.ffn: 20.13 GFLOPs\n",
      "layers.30.ffn.w1: 6.71 GFLOPs\n",
      "layers.30.ffn.w2: 6.71 GFLOPs\n",
      "layers.30.ffn.w3: 6.71 GFLOPs\n",
      "layers.31: 29.53 GFLOPs\n",
      "layers.31.attn: 9.40 GFLOPs\n",
      "layers.31.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.31.attn.output_proj: 1.68 GFLOPs\n",
      "layers.31.ffn: 20.13 GFLOPs\n",
      "layers.31.ffn.w1: 6.71 GFLOPs\n",
      "layers.31.ffn.w2: 6.71 GFLOPs\n",
      "layers.31.ffn.w3: 6.71 GFLOPs\n",
      "layers.32: 29.53 GFLOPs\n",
      "layers.32.attn: 9.40 GFLOPs\n",
      "layers.32.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.32.attn.output_proj: 1.68 GFLOPs\n",
      "layers.32.ffn: 20.13 GFLOPs\n",
      "layers.32.ffn.w1: 6.71 GFLOPs\n",
      "layers.32.ffn.w2: 6.71 GFLOPs\n",
      "layers.32.ffn.w3: 6.71 GFLOPs\n",
      "layers.33: 29.53 GFLOPs\n",
      "layers.33.attn: 9.40 GFLOPs\n",
      "layers.33.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.33.attn.output_proj: 1.68 GFLOPs\n",
      "layers.33.ffn: 20.13 GFLOPs\n",
      "layers.33.ffn.w1: 6.71 GFLOPs\n",
      "layers.33.ffn.w2: 6.71 GFLOPs\n",
      "layers.33.ffn.w3: 6.71 GFLOPs\n",
      "layers.34: 29.53 GFLOPs\n",
      "layers.34.attn: 9.40 GFLOPs\n",
      "layers.34.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.34.attn.output_proj: 1.68 GFLOPs\n",
      "layers.34.ffn: 20.13 GFLOPs\n",
      "layers.34.ffn.w1: 6.71 GFLOPs\n",
      "layers.34.ffn.w2: 6.71 GFLOPs\n",
      "layers.34.ffn.w3: 6.71 GFLOPs\n",
      "layers.35: 29.53 GFLOPs\n",
      "layers.35.attn: 9.40 GFLOPs\n",
      "layers.35.attn.qkv_proj: 5.04 GFLOPs\n",
      "layers.35.attn.output_proj: 1.68 GFLOPs\n",
      "layers.35.ffn: 20.13 GFLOPs\n",
      "layers.35.ffn.w1: 6.71 GFLOPs\n",
      "layers.35.ffn.w2: 6.71 GFLOPs\n",
      "layers.35.ffn.w3: 6.71 GFLOPs\n",
      "lm_head: 65.85 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "for gpt in (model_small, model_med, model_large):\n",
    "    with torch.no_grad():\n",
    "        in_indices = torch.zeros(1, context_length, dtype=torch.int64)\n",
    "        flops = FlopCountAnalysis(gpt, in_indices)\n",
    "        print(f\"FLOPs: {flops.total()/1e9:.2f} GFLOPs\")  # in billions\n",
    "        total_flops = flops.total()\n",
    "        print(f\"lm_head FLOPs ratio: {100 * flops.by_module()[\"lm_head\"] / total_flops:.2f}%\")\n",
    "        for name, num_flops in flops.by_module().items():\n",
    "            gflops = f\"{num_flops/1e9:.2f} GFLOPs\"\n",
    "            if name == \"\":\n",
    "                name = \"TOTAL\"\n",
    "            if num_flops > 0:\n",
    "                print(f\"{name}: {gflops}\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
